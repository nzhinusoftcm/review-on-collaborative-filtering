{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the following link with a new tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nzhinusoftcm/review-on-collaborative-filtering/blob/master/5.MatrixFactorization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MzSv49iMF1_"
   },
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGoHyoipMF2B"
   },
   "source": [
    "<b>User-based</b> and <b>Item-based</b> collaborative Filtering recommender systems suffer from <i>data sparsity</i> and <i>scalability</i> for online recommendations. <b>Matrix Factorization</b> helps to address these drawbacks of memory-based collaborative filtering by reducing the dimension of the rating matrix $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSvTMCyVMF2C"
   },
   "source": [
    "The movielen lasted small dataset has 100k ratings of $m=610$ users on $n=9724$ items. The rating matrix in then a $m\\times n$ matrix (i.e $R\\in \\mathbb{R}^{m\\times n}$). The fact that users usually interact with less than $1\\%$ of items leads the rating matrix $R$ to be highly sparse. For example, the degree of sparsity of the movielen lasted small dataset is \n",
    "\n",
    "\\begin{equation}\n",
    "sparsity = 100 - \\frac{\\text{total # ratings}}{m \\times n} = 100 - \\frac{100000}{610\\times 9724} = 98,3\\%\n",
    "\\end{equation}\n",
    "\n",
    "This means that in this dataset, a user has interacted with less than $2\\%$ of items. To reduce the dimension of the rating matrix $R$, Matrix Factorization (MF) mappes both users and items to a joint latent factor space of dimensionality $k$ such that user-item interactions are modeled as inner products in that space <a href='https://ieeexplore.ieee.org/document/5197422'>(Yehuda Koren et al., 2009)</a>. MF then decomposes $R$ in two matrices as follows :\n",
    "\n",
    "\\begin{equation}\n",
    "R = Q^\\top P\n",
    "\\end{equation}\n",
    "\n",
    "Where $P \\in \\mathbb{R}^{m\\times k}$ represents latent factors of users and $Q \\in \\mathbb{R}^{n\\times k}$ is the latent factors of items. Each line of $P$, say $p_u \\in \\mathbb{R}^k$ denotes the taste of user $u$ and each $q_i \\in \\mathbb{R}^k$ the features of item $i$. The dot product between $p_u$ and $q_i$ will be the rating prediction of user $u$ on item $i$ :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{r}_{u,i} = q_{i}^{\\top} p_u.\n",
    "\\end{equation}\n",
    "\n",
    "Figure 1 presents an example of decomposition of $R$ into two matrices $P$ and $Q$.\n",
    "\n",
    "<img src=\"recsys/img/MF.png\">\n",
    "<br>\n",
    "<center><b>Figure 1</b>: Decomposition of $R$ into $P$ and $Q$</center>\n",
    "\n",
    "\n",
    "To learn the latent factors $p_u$ and $q_i$, the system minimizes the regularized squared error on the set of known ratings. The cost function $J$ is defined as follows : \n",
    "\n",
    "\\begin{equation}\n",
    "J = \\frac{1}{2}\\sum_{(u,i)\\in \\kappa} (r_{ui} - q_{i}^{\\top} p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\kappa$ is the set of $(u,i)$ pairs for which $r_{u,i}$ is known (the training set), and $\\lambda$ is the regularizer parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azVL_VTEMF2C"
   },
   "source": [
    "## Learning Algorithms\n",
    "\n",
    "As described in <a href='https://ieeexplore.ieee.org/document/5197422'>(Yehuda Koren et al., 2009)</a>, to minimize the cost function $J$, the matrix factorization algorithm predicts $\\hat{r}_{u,i}$ for each given training case (existing $r_{u,i}$), and computes the associated error defined by the Mean Absolute Error (MAE) as :\n",
    "\n",
    "\\begin{equation}\n",
    "e_{u,i} = |r_{ui} - q_{i}^{\\top} p_u|.\n",
    "\\end{equation}\n",
    "\n",
    "<b>Note</b> : The overall error $E$ is defined as :\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{M}\\sum_{(u,i)\\in\\kappa} e_{u,i}\n",
    "\\end{equation}\n",
    "\n",
    "Where $M$ is the number of example. The update rules for parameters $p_u$ and $q_i$ are defined as follows :\n",
    "\n",
    "\\begin{equation}\n",
    "q_i \\leftarrow q_i - \\alpha\\frac{\\partial}{\\partial q_i}J_{u,i}, \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p_u \\leftarrow p_u - \\alpha\\frac{\\partial}{\\partial p_u}J_{u,i}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\frac{\\partial}{\\partial p_u}J_{u,i}$ is the partial derivative of the cost function $J$ according to $p_u$. It computes the extent to which $p_u$ contributes to the total error.\n",
    "\n",
    "### How to compute $\\frac{\\partial}{\\partial q_i}J_{u,i}$ ?\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial q_i}J_{u,i} & = & \\frac{1}{2}\\frac{\\partial}{\\partial q_i} \\begin{bmatrix}(r_{ui} - q_{i}^{\\top} p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\\end{bmatrix} \\\\\n",
    "& = & -(r_{u,i}-q_{i}^{\\top} p_u)\\cdot p_u + \\lambda \\cdot q_i  \\\\\n",
    "& = & -e_{u,i}\\cdot p_u+\\lambda \\cdot q_i\n",
    "\\end{align}\n",
    "\n",
    "The update rules are then given by : \n",
    "\n",
    "\\begin{equation}\n",
    "q_i \\leftarrow q_i + \\alpha\\cdot (e_{u,i}\\cdot p_u-\\lambda \\cdot q_i), \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p_u \\leftarrow p_u + \\alpha\\cdot (e_{u,i}\\cdot q_i-\\lambda \\cdot p_u)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04D2zA48MF2D"
   },
   "source": [
    "## Matrix Factorization : algorithm\n",
    "<ol>\n",
    "    <li>Initialize $P$ and $Q$ with random values\n",
    "    <li>For each training example $(u,i)\\in\\kappa$ with the corresponding rating $r_{u,i}$ :\n",
    "        <ul>\n",
    "            <li>compute $\\hat{r}_{u,i}$ as $\\hat{r}_{u,i} = q_{i}^{\\top} p_u$\n",
    "            <li>compute the error : $e_{u,i} = |r_{ui} - \\hat{r}_{u,i}|$\n",
    "            <li>update $p_u$ and $q_i$:\n",
    "                <ul>\n",
    "                    <li>$p_u \\leftarrow p_u + \\alpha\\cdot (e_{u,i}\\cdot q_i-\\lambda \\cdot p_u)$\n",
    "                    <li>$q_i \\leftarrow q_i + \\alpha\\cdot (e_{u,i}\\cdot p_u-\\lambda \\cdot q_i)$\n",
    "                </ul>\n",
    "        </ul>\n",
    "    <li> Repeat step 2 until the optimal parameters are reached.\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download useful files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
    "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
    "    !unzip recsys.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirements\n",
    "```\n",
    "matplotlib==3.2.2\n",
    "numpy==1.19.2\n",
    "pandas==1.0.5\n",
    "python==3.7\n",
    "scikit-learn==0.24.1\n",
    "scikit-surprise==1.1.1\n",
    "scipy==1.6.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_ohlWURMF2F"
   },
   "outputs": [],
   "source": [
    "from recsys.preprocessing import mean_ratings\n",
    "from recsys.preprocessing import normalized_ratings\n",
    "from recsys.preprocessing import ids_encoder\n",
    "from recsys.preprocessing import train_test_split\n",
    "from recsys.preprocessing import rating_matrix\n",
    "from recsys.preprocessing import get_examples\n",
    "from recsys.preprocessing import scale_ratings\n",
    "\n",
    "from recsys.datasets import ml100k\n",
    "from recsys.datasets import ml1m\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    \n",
    "    def __init__(self, m, n, k=10, alpha=0.001, lamb=0.01):\n",
    "        \"\"\"\n",
    "        Initialization of the model        \n",
    "        : param\n",
    "            - m : number of users\n",
    "            - n : number of items\n",
    "            - k : length of latent factor, both for users and items. 50 by default\n",
    "            - alpha : learning rate. 0.001 by default\n",
    "            - lamb : regularizer parameter. 0.02 by default\n",
    "        \"\"\"\n",
    "        np.random.seed(32)\n",
    "        \n",
    "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
    "        self.k = k\n",
    "        self.P = np.random.normal(size=(m, k))\n",
    "        self.Q = np.random.normal(size=(n, k))\n",
    "        \n",
    "        # hyperparameter initialization\n",
    "        self.alpha = alpha\n",
    "        self.lamb = lamb\n",
    "        \n",
    "        # training history\n",
    "        self.history = {\n",
    "            \"epochs\":[],\n",
    "            \"loss\":[],\n",
    "            \"val_loss\":[],\n",
    "            \"lr\":[]\n",
    "        }\n",
    "    \n",
    "    def print_training_parameters(self):\n",
    "        print('Training Matrix Factorization Model ...')\n",
    "        print(f'k={self.k} \\t alpha={self.alpha} \\t lambda={self.lamb}')\n",
    "    \n",
    "    def update_rule(self, u, i, error):\n",
    "        self.P[u] = self.P[u] + self.alpha * (error * self.Q[i] - self.lamb * self.P[u])\n",
    "        self.Q[i] = self.Q[i] + self.alpha * (error * self.P[u] - self.lamb * self.Q[i])\n",
    "        \n",
    "    def mae(self,  x_train, y_train):\n",
    "        \"\"\"\n",
    "        returns the Mean Absolute Error\n",
    "        \"\"\"\n",
    "        # number of training exemples\n",
    "        M = x_train.shape[0]\n",
    "        error = 0\n",
    "        for pair, r in zip(x_train, y_train):\n",
    "            u, i = pair\n",
    "            error += abs(r - np.dot(self.P[u], self.Q[i]))\n",
    "        return error/M\n",
    "    \n",
    "    def print_training_progress(self, epoch, epochs, error, val_error, steps=5):\n",
    "        if epoch == 1 or epoch % steps == 0 :\n",
    "                print(\"epoch {}/{} - loss : {} - val_loss : {}\".format(epoch, epochs, round(error,3), round(val_error,3)))\n",
    "                \n",
    "    def learning_rate_schedule(self, epoch, target_epochs = 20):\n",
    "        if (epoch >= target_epochs) and (epoch % target_epochs == 0):\n",
    "                factor = epoch // target_epochs\n",
    "                self.alpha = self.alpha * (1 / (factor * 20))\n",
    "                print(\"\\nLearning Rate : {}\\n\".format(self.alpha))\n",
    "    \n",
    "    def fit(self, x_train, y_train, validation_data, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train latent factors P and Q according to the training set\n",
    "        \n",
    "        :param\n",
    "            - x_train : training pairs (u,i) for which rating r_ui is known\n",
    "            - y_train : set of ratings r_ui for all training pairs (u,i)\n",
    "            - validation_data : tuple (x_test, y_test)\n",
    "            - epochs : number of time to loop over the entire training set. \n",
    "            1000 epochs by default\n",
    "            \n",
    "        Note that u and i are encoded values of userid and itemid\n",
    "        \"\"\"\n",
    "        self.print_training_parameters()\n",
    "        \n",
    "        # validation data\n",
    "        x_test, y_test = validation_data\n",
    "        \n",
    "        # loop over the number of epochs\n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            # for each pair (u,i) and the corresponding rating r\n",
    "            for pair, r in zip(x_train, y_train):\n",
    "                \n",
    "                # get encoded values of userid and itemid from pair\n",
    "                u,i = pair\n",
    "                \n",
    "                # compute the predicted rating r_hat\n",
    "                r_hat = np.dot(self.P[u], self.Q[i])\n",
    "                \n",
    "                # compute the prediction error\n",
    "                e = abs(r - r_hat)\n",
    "                \n",
    "                # update rules\n",
    "                self.update_rule(u, i, e)\n",
    "                \n",
    "            # training and validation error  after this epochs\n",
    "            error = self.mae(x_train, y_train)\n",
    "            val_error = self.mae(x_test, y_test)\n",
    "            \n",
    "            # update history\n",
    "            self.history['epochs'].append(epoch)\n",
    "            self.history['loss'].append(error)\n",
    "            self.history['val_loss'].append(val_error)\n",
    "            \n",
    "            # update history\n",
    "            self.update_history(epoch, error, val_error)\n",
    "            \n",
    "            # print training progress after each steps epochs\n",
    "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
    "              \n",
    "            # leaning rate scheduler : redure the learning rate as we go deeper in the number of epochs\n",
    "            # self.learning_rate_schedule(epoch)\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def update_history(self, epoch, error, val_error):\n",
    "        self.history['epochs'].append(epoch)\n",
    "        self.history['loss'].append(error)\n",
    "        self.history['val_loss'].append(val_error)\n",
    "        self.history['lr'].append(self.alpha)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        compute the global error on the test set        \n",
    "        :param x_test : test pairs (u,i) for which rating r_ui is known\n",
    "        :param y_test : set of ratings r_ui for all test pairs (u,i)\n",
    "        \"\"\"\n",
    "        error = self.mae(x_test, y_test)\n",
    "        print(f\"validation error : {round(error,3)}\")\n",
    "        \n",
    "        return error\n",
    "      \n",
    "    def predict(self, userid, itemid):\n",
    "        \"\"\"\n",
    "        Make rating prediction for a user on an item\n",
    "        :param userid\n",
    "        :param itemid\n",
    "        :return r : predicted rating\n",
    "        \"\"\"\n",
    "        # encode user and item ids to be able to access their latent factors in\n",
    "        # matrices P and Q\n",
    "        u = uencoder.transform([userid])[0]\n",
    "        i = iencoder.transform([itemid])[0]\n",
    "\n",
    "        # rating prediction using encoded ids. Dot product between P_u and Q_i\n",
    "        r = np.dot(self.P[u], self.Q[i])\n",
    "        return r\n",
    "\n",
    "    def recommend(self, userid, N=30):\n",
    "        \"\"\"\n",
    "        make to N recommendations for a given user\n",
    "\n",
    "        :return(top_items,preds) : top N items with the highest predictions \n",
    "        with their corresponding predictions\n",
    "        \"\"\"\n",
    "        # encode the userid\n",
    "        u = uencoder.transform([userid])[0]\n",
    "\n",
    "        # predictions for users userid on all product\n",
    "        predictions = np.dot(self.P[u], self.Q.T)\n",
    "\n",
    "        # get the indices of the top N predictions\n",
    "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
    "\n",
    "        # decode indices to get their corresponding itemids\n",
    "        top_items = iencoder.inverse_transform(top_idx)\n",
    "\n",
    "        # take corresponding predictions for top N indices\n",
    "        preds = predictions[top_idx]\n",
    "\n",
    "        return top_items, preds        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MovieLens 100k\n",
    "\n",
    "## Evaluation on raw ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ml100k dataset\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
    "\n",
    "m = ratings.userid.nunique()   # total number of users\n",
    "n = ratings.itemid.nunique()   # total number of items\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/10 - loss : 2.734 - val_loss : 2.779\n",
      "epoch 2/10 - loss : 1.764 - val_loss : 1.794\n",
      "epoch 3/10 - loss : 1.592 - val_loss : 1.614\n",
      "epoch 4/10 - loss : 1.538 - val_loss : 1.556\n",
      "epoch 5/10 - loss : 1.515 - val_loss : 1.531\n",
      "epoch 6/10 - loss : 1.503 - val_loss : 1.517\n",
      "epoch 7/10 - loss : 1.496 - val_loss : 1.509\n",
      "epoch 8/10 - loss : 1.491 - val_loss : 1.504\n",
      "epoch 9/10 - loss : 1.488 - val_loss : 1.5\n",
      "epoch 10/10 - loss : 1.486 - val_loss : 1.497\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 1.497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4973507972141993"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on normalized ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
    "\n",
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/10 - loss : 0.851 - val_loss : 0.847\n",
      "epoch 2/10 - loss : 0.831 - val_loss : 0.831\n",
      "epoch 3/10 - loss : 0.828 - val_loss : 0.829\n",
      "epoch 4/10 - loss : 0.827 - val_loss : 0.828\n",
      "epoch 5/10 - loss : 0.827 - val_loss : 0.828\n",
      "epoch 6/10 - loss : 0.826 - val_loss : 0.828\n",
      "epoch 7/10 - loss : 0.826 - val_loss : 0.828\n",
      "epoch 8/10 - loss : 0.826 - val_loss : 0.828\n",
      "epoch 9/10 - loss : 0.826 - val_loss : 0.828\n",
      "epoch 10/10 - loss : 0.826 - val_loss : 0.828\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 0.828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8276982643684648"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MovieLens 1M\n",
    "\n",
    "## Evaluation on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ml1m dataset\n",
    "ratings, movies = ml1m.load()\n",
    "\n",
    "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
    "\n",
    "m = ratings.userid.nunique()   # total number of users\n",
    "n = ratings.itemid.nunique()   # total number of items\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/10 - loss : 1.713 - val_loss : 1.718\n",
      "epoch 2/10 - loss : 1.523 - val_loss : 1.526\n",
      "epoch 3/10 - loss : 1.496 - val_loss : 1.498\n",
      "epoch 4/10 - loss : 1.489 - val_loss : 1.489\n",
      "epoch 5/10 - loss : 1.485 - val_loss : 1.486\n",
      "epoch 6/10 - loss : 1.484 - val_loss : 1.484\n",
      "epoch 7/10 - loss : 1.483 - val_loss : 1.483\n",
      "epoch 8/10 - loss : 1.483 - val_loss : 1.483\n",
      "epoch 9/10 - loss : 1.482 - val_loss : 1.482\n",
      "epoch 10/10 - loss : 1.482 - val_loss : 1.482\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 1.482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4820034560467208"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on normalized ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = ml1m.load()\n",
    "\n",
    "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
    "\n",
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/10 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 2/10 - loss : 0.824 - val_loss : 0.825\n",
      "epoch 3/10 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 4/10 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 5/10 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 6/10 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 7/10 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 8/10 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 9/10 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 10/10 - loss : 0.823 - val_loss : 0.825\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 0.825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8250208634455388"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hzclXP0RL8V"
   },
   "source": [
    "Now that the latent factors $P$ and $Q$, we can use them to make predictions and recommendations. Let's call the ```predict``` function of the ```Matrix Factorization``` class to make prediction for a given.\n",
    "\n",
    "rating prediction for user 1 on item 1 for which the truth rating $r=5.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>itemid</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_mean</th>\n",
       "      <th>norm_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.188679</td>\n",
       "      <td>0.811321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>4.188679</td>\n",
       "      <td>0.811321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>4.188679</td>\n",
       "      <td>0.811321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>254</td>\n",
       "      <td>4</td>\n",
       "      <td>4.188679</td>\n",
       "      <td>-0.188679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>514</td>\n",
       "      <td>5</td>\n",
       "      <td>4.188679</td>\n",
       "      <td>0.811321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  itemid  rating  rating_mean  norm_rating\n",
       "0       1       1       5     4.188679     0.811321\n",
       "1       1      48       5     4.188679     0.811321\n",
       "2       1     145       5     4.188679     0.811321\n",
       "3       1     254       4     4.188679    -0.188679\n",
       "4       1     514       5     4.188679     0.811321"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.userid = uencoder.inverse_transform(ratings.userid.to_list())\n",
    "ratings.itemid = uencoder.inverse_transform(ratings.itemid.to_list())\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WjQN_N1WRKSm",
    "outputId": "0003d90b-3c19-4af8-c77f-5206a60d5be5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.188679163563357"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.188679 + MF.predict(userid=1, itemid=1) # add the mean because we have used the normalised ratings for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYncVXWvebEZ"
   },
   "source": [
    "#### Summary\n",
    "\n",
    "This is the link to the MatrixFactorization class : [MatrixFactorization.py](https://github.com/nzhinusoftcm/review-on-collaborative-filtering/blob/master/recsys/models/MatrixFactorization.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization\n",
    "\n",
    "With the Matrix Factorization model, $P$ and $Q$ latent factors are non interpretable since they contain arbitrary negative or positive values. This make the Matrix Factorization model to be non explainable.\n",
    "\n",
    "The **Non-negative Matrix Factorization (NMF)** algorithm is a variant of Matrix Factorization which generate explainable latent factors for $P$ and $Q$ by constraining their values in the range [0,1]. This allow a probability interpretation of these latent factors, hense the explainability.\n",
    "\n",
    "Click [here](https://github.com/nzhinusoftcm/review-on-collaborative-filtering/blob/master/6.NonNegativeMatrixFactorization.ipynb) to go to the NMF notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-jjxDhex5Gs"
   },
   "source": [
    "## Reference\n",
    "\n",
    "1. Yehuda Koren et al. (2009). <a href='https://ieeexplore.ieee.org/document/5197422'>Matrix Factorization Techniques for Recommender Systems</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jV3Jj17ox_UR"
   },
   "source": [
    "## Author\n",
    "\n",
    "[Carmel WENGA](https://www.linkedin.com/in/carmel-wenga-871876178/), <br>\n",
    "PhD student at Université de la Polynésie Française, <br> \n",
    "Applied Machine Learning Research Engineer, <br>\n",
    "[ShoppingList](https://shoppinglist.cm), NzhinuSoft."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4. matrix factorization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
